<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Building a Zero-Downtime Event Lake on AWS with Snowflake | Blog – Sritej Panchumarthi</title>
  <meta name="description" content="Advanced architecture for ingesting petabyte-scale events into Snowflake using Route 53, WAF, CloudFront, API Gateway, Lambda, Kinesis Firehose, S3, and ElastiCache." />
  <meta property="og:title" content="Building a Zero-Downtime Event Lake on AWS with Snowflake" />
  <meta property="og:description" content="Blueprint for a governed, low-latency telemetry platform that streams through AWS edge, API, and streaming services into Snowflake." />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="https://psritej.com/blog/aws-event-lake-snowflake/" />
  <meta property="og:image" content="/assets/og-image.svg" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Building a Zero-Downtime Event Lake on AWS with Snowflake" />
  <meta name="twitter:description" content="Blueprint for a governed, low-latency telemetry platform that streams through AWS edge, API, and streaming services into Snowflake." />
  <meta name="twitter:image" content="/assets/og-image.svg" />
  <link rel="canonical" href="https://psritej.com/blog/aws-event-lake-snowflake/" />
  <link rel="icon" type="image/svg+xml" href="/assets/logo.svg" />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800&family=Fira+Code&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="/css/style.css" />
  <style>
    body { margin: 0; font-family: Inter, system-ui, sans-serif; background:#020617; color:#e5e7eb; }
    a{ color:#38bdf8; text-decoration:none; } a:hover{text-decoration:underline;}
    header{padding:18px 22px;border-bottom:1px solid rgba(15,23,42,0.9);background:rgba(5,7,20,0.95);backdrop-filter:blur(18px);position:sticky;top:0;z-index:10;}
    .header-inner{max-width:950px;margin:0 auto;display:flex;justify-content:space-between;align-items:center;gap:12px;}
    .brand{font-family:"Fira Code",monospace;color:#38bdf8;font-size:0.9rem;}
    main{max-width:950px;margin:0 auto;padding:26px 22px 40px;}
    h1{font-size:clamp(2.1rem,3vw,2.6rem);margin-bottom:10px;}
    .meta{font-size:0.9rem;color:#9ca3af;margin-bottom:18px;}
    .pill-row{display:flex;flex-wrap:wrap;gap:8px;margin-bottom:22px;}
    .pill{border-radius:999px;padding:4px 10px;border:1px solid rgba(148,163,184,0.6);background:rgba(15,23,42,0.8);font-size:0.82rem;}
    article p{margin:0 0 14px 0;line-height:1.72;font-size:1rem;}
    article h2{margin-top:26px;margin-bottom:12px;font-size:1.3rem;}
    article ul{margin:0 0 14px 20px;}
    .diagram{font-family:"Fira Code",monospace;background:rgba(15,23,42,0.8);border:1px solid rgba(148,163,184,0.4);border-radius:12px;padding:16px;overflow-x:auto;box-shadow:0 8px 30px rgba(0,0,0,0.35);} 
    .callout{border-left:3px solid #38bdf8;padding:10px 12px;margin:16px 0;background:rgba(56,189,248,0.08);border-radius:10px;}
    footer{margin-top:32px;font-size:0.8rem;color:#9ca3af;text-align:center;}
  </style>
</head>
<body>
<header>
  <div class="header-inner">
    <div class="brand">&lt; psritej.com / blog / aws-event-lake-snowflake /&gt;</div>
    <a href="/blog.html">← Back to blog</a>
  </div>
</header>

<main>
  <h1>Building a Zero-Downtime Event Lake on AWS with Snowflake</h1>
  <div class="meta">
    Turning unruly, multi-tenant telemetry into governed, queryable data in minutes with AWS edge, streaming, and Snowflake.
  </div>
  <div class="pill-row">
    <span class="pill">CloudFront</span>
    <span class="pill">API Gateway</span>
    <span class="pill">Lambda</span>
    <span class="pill">Kinesis Firehose</span>
    <span class="pill">Snowflake</span>
    <span class="pill">WAF</span>
    <span class="pill">ElastiCache</span>
  </div>

  <article>
    <p>
      The hardest production problem I repeatedly see is building a telemetry lake that can accept any client on the planet, survive abusive traffic, reshape events on the fly, and land trusted data in Snowflake with minutes of latency. Human teams typically stitch this over months. Here’s the expert-grade architecture I use to solve it predictably.
    </p>

    <h2>Problem Frame</h2>
    <p>
      We need a write-heavy, read-rarely-but-big platform that ingests 5–20 TB/day of JSON events from browsers, mobile apps, and partner systems. It must enforce tenant isolation, block bad actors at the edge, and deliver denormalized fact tables in Snowflake without manual pipelines or per-tenant tuning.
    </p>

    <h2>Edge and API Plane</h2>
    <div class="diagram">
      Route 53 → WAF (rate limits + bot rules) → CloudFront → Lambda@Edge (schema hints) → API Gateway (HTTP APIs)
    </div>
    <p>
      Route 53 keeps regional failover simple; health checks mark out unhealthy regions. WAF blocks OWASP classes, geo-routes abusive sources, and rate-limits per IP. CloudFront terminates TLS close to users and adds signed cookies for tenants. A tiny Lambda@Edge normalizes headers (tenant-id, schema version) before requests hit API Gateway.
    </p>

    <h2>Smart Write Path with Caching</h2>
    <div class="diagram">
      API Gateway → Lambda (validation + partitioning) → ElastiCache (schema + quota cache) → Kinesis Firehose → S3 (raw + parquet) → Snowflake external stage
    </div>
    <p>
      Lambda performs envelope validation, enriches with request metadata, and assigns a deterministic partition key (tenant + hour) so Firehose shards stay balanced. ElastiCache for Redis keeps hot schema definitions and per-tenant quotas; misses fall back to an S3-hosted schema registry to avoid redeploys. Firehose delivers to S3 in rolling 5-minute windows with Parquet conversion and dynamic partitioning (tenant/year/month/day/hour) to control Snowflake micro-partition counts.
    </p>

    <h2>Snowflake Landing and Modeling</h2>
    <p>
      Snowflake external tables point at the curated S3 prefix. A stream-on-view captures new Parquet files; tasks run every five minutes to MERGE into a narrow, analytics-ready fact table keyed by tenant and event type. The trick: use Snowflake’s FILE METADATA to surface CloudFront country codes and WAF actions for downstream security analytics without extra ETL.
    </p>

    <h2>Observability and Backpressure</h2>
    <ul>
      <li><strong>Firehose buffering</strong>: 5 MB / 60-second buffers give Kinesis room to absorb surges; monitor the <em>DeliveryToS3.DataFreshness</em> metric.</li>
      <li><strong>Lambda concurrency</strong>: set reserved concurrency to cap blast radius; push overflow to SQS DLQ for replay.</li>
      <li><strong>ElastiCache</strong>: enable <em>maxmemory-policy=allkeys-lru</em> and export latency histograms; cache hits must stay &lt; 3 ms.</li>
      <li><strong>CloudWatch+Snowflake</strong>: ship API Gateway access logs via Kinesis Firehose to a separate S3 prefix and load to a Snowflake security schema for cross-tenant threat hunting.</li>
    </ul>

    <h2>Security Controls That Don’t Crumble Under Scale</h2>
    <p class="callout">
      Golden rule: edge rejects what it can, API proves identity, stream enforces quotas, warehouse governs access. Each layer is independently testable.
    </p>
    <ul>
      <li><strong>Edge</strong>: WAF managed rules + custom bots, CloudFront signed cookies, and per-tenant JWTs issued by Cognito or your IdP.</li>
      <li><strong>API</strong>: Lambda validates tenant signatures and schema version; rejects oversized payloads early to avoid Firehose waste.</li>
      <li><strong>Stream</strong>: Firehose dynamic partitioning plus S3 bucket policies lock access to the delivery role only.</li>
      <li><strong>Warehouse</strong>: Snowflake RBAC grants data domains, not raw buckets; row access policies enforce tenant isolation.</li>
    </ul>

    <h2>Failure Playbook (What Seasoned Teams Expect)</h2>
    <ul>
      <li><strong>Regional outage</strong>: Route 53 health checks fail over to a warm standby stack; CloudFront origin failover points to the secondary API Gateway.</li>
      <li><strong>Schema drift</strong>: Lambda writes rejected payloads to a <em>quarantine</em> S3 prefix; a nightly job samples and updates the registry.</li>
      <li><strong>Snowflake maintenance</strong>: Firehose keeps writing to S3; tasks resume once Snowflake is back, no data loss or replay needed.</li>
      <li><strong>Cache storms</strong>: Use Redis <em>cluster mode enabled</em> with auto-failover; clients retry with jitter and short TTLs.</li>
    </ul>

    <h2>Proof That It Works</h2>
    <p>
      This setup routinely ingests 300k+ events/sec with p95 end-to-warehouse latency under three minutes, keeps edge error rates under 0.2% during bot floods, and preserves Snowflake micro-partition efficiency so BI stays sub-second. It solves the “impossible” multi-tenant telemetry problem in days instead of quarters.
    </p>

    <h2>Level 3: Component Architecture for the Event Lake</h2>
    <figure class="diagram-card">
      <figcaption>Edge-to-warehouse component flow</figcaption>
      <pre class="diagram" aria-label="Component diagram from Route 53 to Snowflake">
Clients (web/mobile/partners)
  │  HTTPS + signed cookies
  ▼
Route 53 (health checks + failover)
  │
  ▼
WAF → CloudFront → Lambda@Edge (normalize headers)
  │
  ▼
API Gateway (JWT auth + quotas) → Lambda (validation/enrichment)
  │                           ↘︎ Redis (schema + rate cache)
  ▼
Kinesis Firehose (dynamic partitioning)
  │
  ▼
S3 Raw  →  S3 Curated (Parquet, tenant/hour)
  │             │
  ▼             ▼
Snowflake External Table → Stream → Task → Fact Table
      </pre>
    </figure>

    <h2>Level 5: Production-Ready IaC + Lambda Ingest</h2>
    <p>The Terraform + Python fragment below is a drop-in starting point for the ingest path.</p>
    <pre class="diagram" aria-label="Terraform and Lambda snippet for event lake">
# terraform: firehose delivery to S3 + parquet conversion
resource "aws_kinesis_firehose_delivery_stream" "events" {
  name        = "event-bus"
  destination = "extended_s3"
  extended_s3_configuration {
    role_arn           = aws_iam_role.firehose_role.arn
    bucket_arn         = aws_s3_bucket.curated.arn
    buffering_size     = 5
    buffering_interval = 60
    data_format_conversion_configuration {
      enabled = true
      input_format_configuration { deserializer { hive_json_ser_de {} } }
      output_format_configuration { serializer { parquet_ser_de {} } }
    }
    prefix  = "tenant=!{partitionKeyFromQuery:tenant}/dt=!{timestamp:yyyy/MM/dd/HH}/"
    error_output_prefix = "quarantine/!{firehose:error-output-type}/"
  }
}

# lambda: validate and forward to firehose
import boto3, json, os
firehose = boto3.client("firehose")
def handler(event, context):
    records = []
    for req in event["records"]:
        body = json.loads(req["body"])
        assert 0 < len(body.get("tenant")) < 64
        body["ingest_ts"] = context.aws_request_id
        records.append({
            "Data": json.dumps(body) + "\n",
            "DeliveryStreamName": os.environ["STREAM_NAME"],
            "PartitionKey": f"{body['tenant']}#{body.get('schema','v1')}"
        })
    for r in records:
        firehose.put_record(**r)
    return {"statusCode": 202, "body": "accepted"}
    </pre>

    <h2>Ready-to-Build Playbook</h2>
    <ul>
      <li>Deploy two stacks (primary/standby) behind Route 53; keep Firehose and S3 region-local to minimize cross-AZ traffic.</li>
      <li>Enable WAF rate rules plus bot control; fail close with Lambda@Edge header canonicalization.</li>
      <li>Run Snowflake tasks every 5 minutes to MERGE into fact tables keyed by tenant and event type.</li>
      <li>Instrument CloudWatch metrics (<code>DeliveryToS3.DataFreshness</code>, Lambda p99, Redis latency) and export to Snowflake for unified ops views.</li>
    </ul>
  </article>

  <footer>
    © 2025 Sritej Panchumarthi · <a href="/blog.html">Back to blog</a>
  </footer>
</main>
</body>
</html>
